---
Title: "Staroverova_DKR2_PSA-3"
output: word_document
lang: russian
---

Для анализа будем использовать дата-сет с выборкой 2000 наблюдений о мобильных телефонах. Телефоны разделены на группы по ценовому признаку (price_range группы 0-3 в порядке увеличения их стоимости). Загрузим данные как csv-файл.

[For this analysis will be used subset data on mobile phones with 2000 observations. Phones are divided on 4 groups using price range (from 0 to 3 in in ascending order of price. We are reading this data as csv-file)]

```{r}
library(utils)
data_phones <- read.csv(file.choose())
```
```{r}
summary(data_phones)
nrow(data_phones)
```

```{r}
library(ggplot2)
ggplot(data_phones, aes(x=ram, y=battery_power, group = price_range))  + theme_classic()+
  geom_smooth(method=lm, se = TRUE, color = "black") + geom_point() + facet_wrap(~price_range)

```
Интересно, что для всех ценовых категорий можно найти телефоны в разном промежутке вместимости батареи, хотя на гистограмме очевидно увеличение среднего объема оперативной памяти с увеличением класса цены телефона

[A notable feature on this histogram is that it can be found phones in any battery power range both in the low price category and in the high, however can be seen a dynamics on increasing ram with increasing price category. The most important predictors are discribed bottom:]

Как наиболее значимые (как потенциально наиболее значимые для цены телефоне) логически переменные выделим:
battery_power - вместимость батареи (непрерывная)
ram - объем опретивной памяти (непрерывная)
int_memory - объем памяти телефона (непрерывная)
px_height - высота экрана в пикселях (непрерывная)
px_width - ширина экрана в пикселях (непрерывная)
n_cores - поколение процессора (непрерывная)
touch_screen - бинарная переменная: сенсорный экран есть - 1, иначе - 0
wifi - бинарная переменная: подключение к wifi есть - 1, иначе - 0



Данную выборку разобьем на тринировочную (n train = 1500 наблюдений) и тестовую (n test = 500 наблюдений).
[I am dividing data on train and test subsets as 3:1 proportion]
```{r}
data_phones <- subset(data_phones, select = c(battery_power, ram, int_memory, mobile_wt, px_height, px_width, n_cores,touch_screen, wifi, price_range))
data_phones <- cbind(data_phones, c(1:2000))
number <- data_phones$`c(1:2000)`
train <- (number <= 1500)
test <- (number > 1500)
summary(data_phones)
```
Данные не скоррелированы:

[There is only one significant correlation score - berween ram and pritce-range. We make a conclusion that ram will be our the most powerful predictor]
```{r}
cor <- cor(as.matrix(train))
cor
library(corrplot)
corrplot(cor)
cor2 <- cor(data_phones)
corrplot(cor2)
```
```{r}
hist(data_phones$ram)
```


Большинство параметров имеют равномерное распределение. Применение логарифмирования для стандартизированных данных помогает несколько исправить ситуацию:
[Now I`ll try to transform my data to more normal-looking distribution]
```{r}
hist(data_phones$battery_power)

for (i in 1:7){
  data_phones[, i] <- log(data_phones[, i])
}

for (i in 1:7){
  data_phones[, i] <- scale(data_phones[, i])
}

hist(data_phones$battery_power)
```

```{r}
hist(data_phones$ram)
```


Однако по формальным тестам параметры нельзя назвать нормально распределенными.
[However even after log-transformation we cann`t say having a normal distribution due to Jarque-Bera test (it takes thrird and forth momentum of normal distribution in its null hypothesis)]
```{r}
#normality test
library(normtest)
jb.norm.test(data_phones$battery_power)
```
Проверим Гипотезу о равных ковариационных матрицах для подгрупп телефонов в разных ценовых сегментах. Получаем, стого говоря, данные, которые можно использовать для дискриминантного анализа со строгой гипотезой.

[Testing Hypothesis on equal covariation-matrix for different price ranges. Since the confidence intervals intersect, our data is suitable for discriminant analysis]

```{r}
library(heplots)
boxM <- boxM(as.matrix(data_phones[,c("battery_power", "ram", "int_memory", "mobile_wt", "px_height", "px_width", "n_cores","touch_screen", "wifi")]), group = data_phones$price_range)
boxM
plot(boxM)
```
Проведем дискриминационный анализ:
[discriminant analysis]
```{r}
library(MASS)
lda <- lda(price_range ~ battery_power+ ram+ int_memory+ mobile_wt+ px_height+ px_width+ n_cores+touch_screen+ wifi, data = data_phones, subset = train)
lda
plot(lda)
```
Попробуем предсказать ответы с помощью нашей модели для тестовой выборки. Посмотрим основную характеристику качества (т.к. данные равнораспределены) - долю верно предсказанных классов. Итак, модель линейного дискриминантного анализа верно  предсказывает 93,8% наблюдений.

[As we can see, precision score is about 93.8% - and the good thing is that our model is`t more likely to predict higher or lower price preferably - we don`t have bias]

```{r}
lda.pred <- predict(lda, data_phones[test,])
names(lda.pred)

lda.class <- lda.pred$class
table(lda.class, data_phones$price_range[test])

mean(lda.class == data_phones$price_range[test])
```
Если ослабить предпосылку о равных ковариационных матрицах, обучим на приведенных данных модель дискриминантного анализа с квадратичным классификатором:
[If we weaken the assumption about equal covariance matrices:]

```{r}
qda.fit <- qda(price_range ~ battery_power+ ram+ int_memory+ mobile_wt+ px_height+ px_width+ n_cores+touch_screen+ wifi, data = data_phones, subset = train)
qda.fit
```
Можно заметить увеличение среднего значения представленных характеристик телефонов с увеличением ценовой категории этих телефонов почти во всех случаях. Исключения - int_memory, n_cores, touch_screen и wifi - последние 3 - характеристики, которые сейчас присутствуют почти во все современных моделях телефонов

[You can notice an increase in the average value of the presented characteristics of phones with an increase in the price category of these phones in almost all cases. Exceptions - int_memory, n_cores, touch_screen and wifi - the last 3 are characteristics that are now present in almost all modern phones]

Проверим качество предсказаний для новой модели: 
[Lets check the presicion score for new (non-equal covar-matrix) model:]

92% верно предсказанных ответов говорит о том, что качество модели немного ухудшилось. Однако это может быть связано с объемом наблюдений в каждой из выборок (переучилась модель) или с наличием в тестовой выборке выбросов, тк. процент ухудшениия модели незначительный.


[92% of correctly predicted answers indicate that the quality of the model has slightly deteriorated. However, this may occurs due to the number of observations in each of the samples (the model was retrained) or to the presence of outliers in the test sample, as the percentage of model deterioration is insignificant.]

```{r}
qda.pred <- predict(qda.fit, data_phones[test,])
qda.class <- qda.pred$class
table(qda.class, data_phones$price_range[test])

mean(qda.class == data_phones$price_range[test])
```

Итак, в помощью моделей дискриминантного анализа на данных с характеристиками телефонов мы можем в достаточно высокой точностью (около 94%) предсказывать ценовую категорию новых устройств. Несмотря на то, что сами параметры не коррелируют между собой, результат получается достаточно однозначным. 

[So, using discriminant analysis models on data with the characteristics of phones, we can predict the price category of new devices with a fairly high accuracy (about 94%). Despite the fact that the parameters themselves do not correlate with each other, the result is quite unambiguous.]


Теперь аналогичную задачу решим с помощью метода решающих деревьев
[Now lets try to solve this task with decision trees]

```{r}
library(rpart)
library(rpart.plot)
library(tree)
library(htmlTable)
library(MLmetrics)
library(randomForest)
```



```{r}
tree <- rpart(data=data_phones, price_range ~ battery_power + ram + int_memory+ mobile_wt + px_height + px_width+n_cores + touch_screen + wifi, method = "class")
tree
```
```{r}
summary(tree)
```
Как и ожидалось, объем оперативной памяти стал наиболее важной переменной в анализе
[As we expected, ram was the most significant value for analysis]

```{r}
plot(tree)
text(tree, pretty=0,cex=0.6)
```
```{r}
set.seed(123)

train_data <- data_phones[train,] 

test_data <- data_phones[test,] 

tree.data_phones <- rpart(data=train_data, price_range ~ battery_power + ram + int_memory+ mobile_wt + px_height + px_width+n_cores + touch_screen + wifi) 

tree.pred <- round(predict(tree.data_phones, test_data))

table(tree.pred, test_data$price_range)
```


```{r}
misclass <- function(pred, obs){tbl <- table(pred, obs)
sum <- colSums(tbl)
dia <- diag(tbl)
msc <- ((sum - dia)/sum) * 100
m.m <- mean(msc)
cat("Classification table:", "\n")
print(tbl)
cat("Misclassification errors:", "\n")
print(round(msc, 2))

print(round(m.m, 2))}

misclass(tree.pred, test_data$price_range)
```

Как мы знаем, отдельные деревья отличаются наличием высоких отклонений от среднего значения, что не есть хорошо для нашей задачи. В связи с этим применим метод случайного леса для имеющихся данных

[As we know, individual trees differ in the presence of high deviations from the mean, which is not good for our task. In this regard, we can apply the random forest method for the available data]

```{r}
set.seed (123)
bag.price <- randomForest(data=train_data, price_range ~ battery_power + ram + int_memory+ mobile_wt + px_height + px_width+n_cores + touch_screen + wifi, mtry = ncol(train_data)-1, importance = TRUE)
bag.price
```

Finally, our random forest reached a peak of 94.4% of explained variation! 


The last thing, lets check the difference if we aplly discriminant analysis to our data as a regression task (not classification)

```{r}
set.seed(123)
sample <- sample.int(n = nrow(data_phones), size = floor(0.75*nrow(data_phones)), replace = F)
train_data_2 <- data_phones[sample, ]
test_data_2  <- data_phones[-sample, ]
```


```{r}
tree <- rpart(data=train_data_2, price_range ~ battery_power + ram + int_memory+ mobile_wt + px_height + px_width+n_cores + touch_screen + wifi, method = 'anova') # anova т.к. целевая переменная вещественная
tree
```

```{r}
rpart.plot(tree)
```

Here we see 1 less leafs allocated!


